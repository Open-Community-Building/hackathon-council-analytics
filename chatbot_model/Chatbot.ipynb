{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC5-5Pg1Wd4d"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pypdf langchain_community PyPDF2 nltk pymupdf transformers pinecone-client chromadb bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers bitsandbytes"
      ],
      "metadata": {
        "id": "xCr0UYruWi2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import PyPDF2"
      ],
      "metadata": {
        "id": "Na0aOPYzWi0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read PDF texts"
      ],
      "metadata": {
        "id": "9I9WstR2cyHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import os\n",
        "\n",
        "# Function to extract text from a single PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    # Initialize an empty string to store extracted text\n",
        "    text = \"\"\n",
        "\n",
        "    # Loop through each page and extract text\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        text += page.get_text(\"text\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# Folder containing the PDF files\n",
        "pdf_folder = 'pdf'\n",
        "\n",
        "# List to store the text from each PDF file\n",
        "pdf_texts = []\n",
        "\n",
        "# Loop through all PDF files in the folder\n",
        "for filename in os.listdir(pdf_folder):\n",
        "    if filename.endswith('.pdf'):\n",
        "        # Construct full file path\n",
        "        pdf_path = os.path.join(pdf_folder, filename)\n",
        "\n",
        "        # Extract text from the current PDF file\n",
        "        extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Append the extracted text to the list\n",
        "        pdf_texts.append(extracted_text)\n",
        "\n",
        "if pdf_texts:\n",
        "    print(\"Text from first PDF file:\")\n",
        "    print(pdf_texts[0][:1000])\n",
        "    print(\"\\n--- End of text preview ---\\n\")\n"
      ],
      "metadata": {
        "id": "QoDlCWAmWiw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize"
      ],
      "metadata": {
        "id": "7QmA008Ac0xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from langchain.embeddings.base import Embeddings\n",
        "import torch\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=150):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = [' '.join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "class CustomEmbeddings(Embeddings):\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return [self.embed_query(text) for text in texts]\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        return embeddings.squeeze().numpy()\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Ensure the padding token is correctly set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Initialize the custom embedding function with BERT model\n",
        "embedding_function = CustomEmbeddings(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Directory containing the text files\n",
        "directory = '/content/sample_data/text'\n",
        "\n",
        "text_lst= []\n",
        "\n",
        "# Loop over multiple text files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Read the contents of the file\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            pdf_text = file.read()\n",
        "\n",
        "        # Split the text into chunks\n",
        "        chunks = split_text_into_chunks(pdf_text)\n",
        "\n",
        "        # Vectorize each chunk using the custom embedding function\n",
        "        vectors = [embedding_function.embed_query(chunk) for chunk in chunks]\n",
        "\n",
        "        text_lst.append(vectors)\n",
        "\n",
        "        # Print or store the resulting vectors\n",
        "        print(f\"Vectors for {filename}:\")\n",
        "        for idx, vector in enumerate(vectors):\n",
        "            print(f\"Vector {idx+1}: {vector}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FCptl5cmWiub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pinecone initialization and upsert"
      ],
      "metadata": {
        "id": "EYtK0gYlc6DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import pinecone\n",
        "\n",
        "# # Initialize Pinecone\n",
        "# pinecone_client = Pinecone(api_key=\"76e72a0a-c875-418d-bf24-d1da16967c6a\")\n",
        "\n",
        "\n",
        "# Create an index\n",
        "# pinecone_client.create_index(\n",
        "#     name=\"stadtlabor\",\n",
        "#     dimension=768,  # Set this to the dimensionality of your vectors\n",
        "#     metric=\"cosine\",  # Or \"euclidean\", depending on your use case\n",
        "#     spec=ServerlessSpec(\n",
        "#         cloud='aws',\n",
        "#         region='us-east-1'  # Choose your region\n",
        "#     )\n",
        "# )\n",
        "\n",
        "\n",
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"76e72a0a-c875-418d-bf24-d1da16967c6a\")\n",
        "index = pc.Index(\"hackathon\")"
      ],
      "metadata": {
        "id": "LY-k7P0CWw8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, vector in enumerate(vectors):\n",
        "    vector_list = vector.tolist()\n",
        "    index.upsert([{\"id\": str(i), 'values' : vector_list, 'metadata': {'text': chunks[i]}}])\n",
        "\n"
      ],
      "metadata": {
        "id": "TwMNHG9CWw5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEKI2mSUWw22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama initialization"
      ],
      "metadata": {
        "id": "IE9Z_exRc_Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is still remaining."
      ],
      "metadata": {
        "id": "UwbCsH-idDfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaModel, BitsAndBytesConfig\n",
        "from torch import cuda, bfloat16\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "\n",
        "token = \"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\"\n",
        "tokenizer = AutoTokenizer.from_pretrained( \"meta-llama/Llama-3.1-8B\", use_auth_token= token)\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype= bfloat16\n",
        ")\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,  # Use 4-bit quantization\n",
        "#     bnb_4bit_use_double_quant=True,  # Optional: Use double quantization\n",
        "#     bnb_4bit_quant_type=\"fp4\"  # Optional: Set quantization type (fp4 for faster performance)\n",
        "# )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.1-8B\",\n",
        "    use_auth_token= token,\n",
        "    quantization_config=bnb_config,  # Use 'int8' or 'int4' for quantization\n",
        "    # device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "e8YullgRWirs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the model\n",
        "# model_save_path = 'quantized_llama_model.pt'  # specify your desired file path\n",
        "\n",
        "# # Save the state_dict of the model\n",
        "# torch.save(model.state_dict(), model_save_path)\n",
        "# print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "pEMzd9yhYJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "\n",
        "# embedding_model = HuggingFaceEmbeddings(model_name=\"meta-llama/Llama-3.1-8B\")\n",
        "\n",
        "vector_store = Pinecone(index=index, embedding=embedding_function, text_key= 'text')\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Based on the following context, answer the question: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline=pipeline(\"text-generation\", model=model, tokenizer=tokenizer))\n",
        "\n",
        "llm_chain = LLMChain(llm=HuggingFacePipeline(pipeline=pipeline(\"text-generation\", model=model, tokenizer=tokenizer)),\n",
        "                     prompt=prompt_template)\n",
        "\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm_chain,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    # prompt=prompt_template\n",
        ")\n",
        "\n",
        "query = \"What is this document about?\"\n",
        "results = rag_chain( \"What is this document about?\")\n",
        "\n",
        "print(\"Answer:\", results['result'])\n",
        "\n",
        "# for doc in results['source_documents']:\n",
        "#     print(f\"Source Document: {doc.metadata['source']}, Score: {doc.score}\")"
      ],
      "metadata": {
        "id": "RzwX-N8KWip0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oB5qBzxWWinO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zivs4ilMWikr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}